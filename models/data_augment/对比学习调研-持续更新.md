## 对比学习调研-持续更新
### 思想

通过自动构造正负样本，要求模型得到一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似
的实例在投影空间中距离比较远。

归纳三部走：

- 通过一些方式构造人工正样本对；
- 在一个Batch内构造负样本对；
- 设计一个loss，拉近正样本对embedding间的距离，扩大负样本对embedding间的距离，距离可使用余弦距离。

### 对齐性和均匀性
好的对比学习模型具备两个属性：对齐性、均匀性。

#### 对齐性
相似的例子，映射到单位超球面后，应该有接近的特征，也就是说，在超平面上距离比较近。
#### 均匀性
模型应该倾向在特征里保留尽可能多的信息，等价于映射到单位超球面的特征应尽可能均匀分布在球面上，分布得越均匀，意味着保留的信息越充分，可直接用线性分类器划分。


### SimCSE

#### 抛出一个数据增强问题
样本正例增强：传统方法有 上下采样、EDA、回译、同义词替换等方法，不仅费时构建，还很可能改变原句意思，降低模型效果。

负样本构建：人工构建负样本难，需要大量实验成本。

总之，不管正样本还是负样本，基于人工或者传统方法都不好构建。
#### SimCSE如何解决数据问题
很简单，一个batch内，一个样本经过两次encoder，得到该样本的正例，负样本则是同一batch内其它的样本。

#### 如何训练

1. 基于bert系列预训练模型训练；

2. 一个batch经复制后以2*batch的数据量喂给模型；

3. 构建一个损失函数infoNCE loss，思想是去最大化正例对的相似度，最小化负例对的相似度；

4. 损失计算：y_pre为句子和batch内其它句子计算的cos相似度值，y_true为被拿来计算句子的索引；

本质是让模型在训练过程中拉近正样本间的距离，推远负样本间的距离。


#### 模型结果

1、语义文本相似度(Semantic textual similarity)

![Semantic textual similarity](https://pic4.zhimg.com/80/v2-fe7e130f0179f57ea4d5e2bb594edf77_720w.jpg)

可以看出在STS任务上，SimCES表现非常好，在所有任务上取得SORT，而却仅仅是无监督方法就超越了有监督。

2、 下游任务(fine turing)

![](https://pic4.zhimg.com/80/v2-3058ea4c9324b8563ebcbcc17dc08b63_720w.jpg)

下游任务并没有做到最好，句子级别的任务可能并不会有益于下游任务训练。 

下游任务往往是一个有监督的任务，有监督就代表蕴含人的主观性，如我很高兴 和 我不高兴，去除人的主观因素，这两句话的相似度很高的，但是加了人的主观定义在里面
这两句话意思是截然相反的，而simCSE只是在无监督数据上通过自学习客观提供一个好的句向量表示模型，并不会益于下游任务。

所以SimCSE适合作为辅助模型，而核心业务如意图识别等还是要有标注的有监督模型来训练。

### BERT-WHITENING

一个线性变换，就可以得到更好的bert句向量表示。

#### 抛出一个cos问题
![](https://pic3.zhimg.com/80/v2-5d02dca35fade98882f2bd76bd387d7e_720w.webp)

我们知道，cos可以用来计算两个文本向量的相似度，但前提是公式只在 标准正交基 下成立，如果基底不同，那么计算
cos相似度的公式就不一样。

所以BERT的CLS向量为什么在文本语义计算上表现很差，很可能是此时的句向量处于一个 非标准正交基(斜着的坐标)，自然不能用 标准正交基
下的cos来计算相似度。

#### 如何解决这个问题

原作者用了大量公式来推理，但是原理很简单。

我们知道标准正态分布的均值为0，协方差矩阵为单位阵 (标准正交基)，那么我们可以学习一个参数W，将BERT句向量分布变换成均值为0，协方差矩阵为单位阵就可以了
，这就是BERT-WHITENING模型的思路。

#### 还能增效又提速

作者发现变换后的句子向量矩阵，经过PCA(只保留投影较大的维度)降维，作者实验时将bert-base 768 降至 256维，实验模型结果有轻微提高， 速度大大增加。

